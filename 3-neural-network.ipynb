{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industrial Process Metrics Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to develop a simple neural network model for the task. We will use a single model to predict all the 9 metrics (we droped the 2 constant metrics in the last step).\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Develop an empirical model of the industrial process using AI.\n",
    "- Achieve millisecond response times for the model.\n",
    "- Ensure a maximum error of 0.2% compared to the digital twin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "X_val = pd.read_csv('data/processed/X_val.csv')\n",
    "y_val = pd.read_csv('data/processed/y_val.csv')\n",
    "X_test = pd.read_csv('data/processed/X_test.csv')\n",
    "y_test = pd.read_csv('data/processed/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['vazaoVapor', 'pressaoVapor', 'temperaturaVapor',\n",
    "            'cargaVaporTG1', 'cargaVaporTG2', 'habilitaTG1', 'habilitaTG2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['consumoEspecificoTG1_1', 'consumoEspecificoTG1_2',\n",
    "           'consumoEspecificoTG2_1', 'consumoEspecificoTG2_2',\n",
    "           'potenciaGeradaTG1_1', 'potenciaGeradaTG1_2',\n",
    "           'potenciaGeradaTG2_1', 'potenciaGeradaTG2_2',\n",
    "           'vazaoVaporEscape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "consumoEspecificoTG1_1    129630\n",
       "consumoEspecificoTG1_2    129630\n",
       "consumoEspecificoTG2_1    131166\n",
       "consumoEspecificoTG2_2    131166\n",
       "potenciaGeradaTG1_1       129630\n",
       "potenciaGeradaTG1_2       129630\n",
       "potenciaGeradaTG2_1       131166\n",
       "potenciaGeradaTG2_2       131166\n",
       "vazaoVaporEscape               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_zeros_count = (y_train == 0).sum()\n",
    "target_zeros_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "consumoEspecificoTG1_1    0.500228\n",
       "consumoEspecificoTG1_2    0.500228\n",
       "consumoEspecificoTG2_1    0.506155\n",
       "consumoEspecificoTG2_2    0.506155\n",
       "potenciaGeradaTG1_1       0.500228\n",
       "potenciaGeradaTG1_2       0.500228\n",
       "potenciaGeradaTG2_1       0.506155\n",
       "potenciaGeradaTG2_2       0.506155\n",
       "vazaoVaporEscape          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of zeros in the target\n",
    "target_zeros_count / y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_zeros = target_zeros_count[target_zeros_count > 0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# To save the models\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "model_checkpoint_cb = ModelCheckpoint('./models/dense_64_32_11_mse_v1.keras', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean_columns = ['habilitaTG1', 'habilitaTG2']\n",
    "low_cardinality_columns = [col for col in X_train.columns if X_train[col].nunique() < 15 and col not in boolean_columns]\n",
    "low_cardinality_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load model: Exception('Forçando novo treinamento')\n"
     ]
    }
   ],
   "source": [
    "new_train = True\n",
    "model_name = 'dense_64_32_11_mse_v1.1.keras'\n",
    "\n",
    "try:\n",
    "    if new_train:\n",
    "        raise Exception('Forçando novo treinamento')\n",
    "    model = load_model(f'./models/{model_name}')\n",
    "    print('Pre-trained model loaded successfully')\n",
    "except Exception as e:\n",
    "    print('Could not load model:', e.__repr__())\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(len(features),)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(targets)))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m297\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,889</span> (11.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,889\u001b[0m (11.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,889</span> (11.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,889\u001b[0m (11.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "model_checkpoint_cb = ModelCheckpoint(f'./models/{model_name}', monitor='val_loss', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler\n",
    "joblib.dump(scaler, './models/scaler_NN.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m8049/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 586us/step - loss: 1850.6892\n",
      "Epoch 1: val_loss improved from inf to 30.84571, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 626us/step - loss: 1841.8365 - val_loss: 30.8457\n",
      "Epoch 2/1000\n",
      "\u001b[1m8040/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 585us/step - loss: 36.6205\n",
      "Epoch 2: val_loss improved from 30.84571 to 25.84392, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 617us/step - loss: 36.5381 - val_loss: 25.8439\n",
      "Epoch 3/1000\n",
      "\u001b[1m8066/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 32.1707\n",
      "Epoch 3: val_loss improved from 25.84392 to 23.47475, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 654us/step - loss: 32.1278 - val_loss: 23.4747\n",
      "Epoch 4/1000\n",
      "\u001b[1m8023/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 568us/step - loss: 30.1272\n",
      "Epoch 4: val_loss improved from 23.47475 to 22.19869, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 593us/step - loss: 30.0343 - val_loss: 22.1987\n",
      "Epoch 5/1000\n",
      "\u001b[1m8071/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 590us/step - loss: 28.9070\n",
      "Epoch 5: val_loss improved from 22.19869 to 21.24127, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 614us/step - loss: 28.8729 - val_loss: 21.2413\n",
      "Epoch 6/1000\n",
      "\u001b[1m8009/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 574us/step - loss: 28.1062\n",
      "Epoch 6: val_loss improved from 21.24127 to 20.47110, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 599us/step - loss: 28.0006 - val_loss: 20.4711\n",
      "Epoch 7/1000\n",
      "\u001b[1m8045/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 591us/step - loss: 27.3971\n",
      "Epoch 7: val_loss improved from 20.47110 to 19.79738, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 618us/step - loss: 27.3342 - val_loss: 19.7974\n",
      "Epoch 8/1000\n",
      "\u001b[1m8041/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 26.9021\n",
      "Epoch 8: val_loss improved from 19.79738 to 19.11176, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 719us/step - loss: 26.8353 - val_loss: 19.1118\n",
      "Epoch 9/1000\n",
      "\u001b[1m8086/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 728us/step - loss: 26.3004\n",
      "Epoch 9: val_loss improved from 19.11176 to 18.70685, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 759us/step - loss: 26.2847 - val_loss: 18.7069\n",
      "Epoch 10/1000\n",
      "\u001b[1m8093/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 610us/step - loss: 25.8968\n",
      "Epoch 10: val_loss improved from 18.70685 to 18.17338, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 636us/step - loss: 25.8890 - val_loss: 18.1734\n",
      "Epoch 11/1000\n",
      "\u001b[1m8076/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 25.5123\n",
      "Epoch 11: val_loss improved from 18.17338 to 17.78563, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 660us/step - loss: 25.4858 - val_loss: 17.7856\n",
      "Epoch 12/1000\n",
      "\u001b[1m8091/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 25.1694\n",
      "Epoch 12: val_loss improved from 17.78563 to 17.40176, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 626us/step - loss: 25.1595 - val_loss: 17.4018\n",
      "Epoch 13/1000\n",
      "\u001b[1m8033/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 611us/step - loss: 24.9449\n",
      "Epoch 13: val_loss improved from 17.40176 to 17.26569, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 637us/step - loss: 24.8718 - val_loss: 17.2657\n",
      "Epoch 14/1000\n",
      "\u001b[1m8076/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 24.7118\n",
      "Epoch 14: val_loss improved from 17.26569 to 17.08392, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 630us/step - loss: 24.6856 - val_loss: 17.0839\n",
      "Epoch 15/1000\n",
      "\u001b[1m8032/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 24.4576\n",
      "Epoch 15: val_loss improved from 17.08392 to 16.50250, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 641us/step - loss: 24.3843 - val_loss: 16.5025\n",
      "Epoch 16/1000\n",
      "\u001b[1m8043/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 24.2425\n",
      "Epoch 16: val_loss improved from 16.50250 to 16.42973, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 653us/step - loss: 24.1812 - val_loss: 16.4297\n",
      "Epoch 17/1000\n",
      "\u001b[1m8025/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 24.0715\n",
      "Epoch 17: val_loss improved from 16.42973 to 15.92140, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 656us/step - loss: 23.9911 - val_loss: 15.9214\n",
      "Epoch 18/1000\n",
      "\u001b[1m8070/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 23.8119\n",
      "Epoch 18: val_loss improved from 15.92140 to 15.58910, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 684us/step - loss: 23.7799 - val_loss: 15.5891\n",
      "Epoch 19/1000\n",
      "\u001b[1m8032/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 23.6459\n",
      "Epoch 19: val_loss did not improve from 15.58910\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 646us/step - loss: 23.5739 - val_loss: 15.5895\n",
      "Epoch 20/1000\n",
      "\u001b[1m8082/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 23.5113\n",
      "Epoch 20: val_loss did not improve from 15.58910\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 672us/step - loss: 23.4922 - val_loss: 15.6657\n",
      "Epoch 21/1000\n",
      "\u001b[1m8051/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 23.2940\n",
      "Epoch 21: val_loss improved from 15.58910 to 15.44495, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 662us/step - loss: 23.2426 - val_loss: 15.4449\n",
      "Epoch 22/1000\n",
      "\u001b[1m8021/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 617us/step - loss: 23.3028\n",
      "Epoch 22: val_loss improved from 15.44495 to 15.23490, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 646us/step - loss: 23.2198 - val_loss: 15.2349\n",
      "Epoch 23/1000\n",
      "\u001b[1m8064/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 23.0039\n",
      "Epoch 23: val_loss improved from 15.23490 to 14.95600, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 670us/step - loss: 22.9664 - val_loss: 14.9560\n",
      "Epoch 24/1000\n",
      "\u001b[1m8018/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 23.0800\n",
      "Epoch 24: val_loss improved from 14.95600 to 14.60680, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 676us/step - loss: 22.9946 - val_loss: 14.6068\n",
      "Epoch 25/1000\n",
      "\u001b[1m8048/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 22.7890\n",
      "Epoch 25: val_loss did not improve from 14.60680\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 688us/step - loss: 22.7354 - val_loss: 14.6806\n",
      "Epoch 26/1000\n",
      "\u001b[1m8025/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 22.8404\n",
      "Epoch 26: val_loss improved from 14.60680 to 14.33844, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 712us/step - loss: 22.7629 - val_loss: 14.3384\n",
      "Epoch 27/1000\n",
      "\u001b[1m8041/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 604us/step - loss: 22.5770\n",
      "Epoch 27: val_loss did not improve from 14.33844\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 632us/step - loss: 22.5165 - val_loss: 14.5231\n",
      "Epoch 28/1000\n",
      "\u001b[1m8026/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 22.4347\n",
      "Epoch 28: val_loss improved from 14.33844 to 14.23380, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 666us/step - loss: 22.3596 - val_loss: 14.2338\n",
      "Epoch 29/1000\n",
      "\u001b[1m8060/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 22.5370\n",
      "Epoch 29: val_loss improved from 14.23380 to 14.07522, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 668us/step - loss: 22.4961 - val_loss: 14.0752\n",
      "Epoch 30/1000\n",
      "\u001b[1m8068/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 601us/step - loss: 22.3735\n",
      "Epoch 30: val_loss improved from 14.07522 to 13.68384, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 641us/step - loss: 22.3409 - val_loss: 13.6838\n",
      "Epoch 31/1000\n",
      "\u001b[1m8091/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 733us/step - loss: 22.2057\n",
      "Epoch 31: val_loss improved from 13.68384 to 13.65622, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 763us/step - loss: 22.1966 - val_loss: 13.6562\n",
      "Epoch 32/1000\n",
      "\u001b[1m8044/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 22.0595\n",
      "Epoch 32: val_loss did not improve from 13.65622\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 655us/step - loss: 22.0034 - val_loss: 13.9608\n",
      "Epoch 33/1000\n",
      "\u001b[1m8092/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 22.2007\n",
      "Epoch 33: val_loss improved from 13.65622 to 13.63616, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 759us/step - loss: 22.1926 - val_loss: 13.6362\n",
      "Epoch 34/1000\n",
      "\u001b[1m8047/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 21.9010\n",
      "Epoch 34: val_loss improved from 13.63616 to 13.41354, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 666us/step - loss: 21.8484 - val_loss: 13.4135\n",
      "Epoch 35/1000\n",
      "\u001b[1m8037/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 22.0097\n",
      "Epoch 35: val_loss improved from 13.41354 to 13.37655, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 693us/step - loss: 21.9465 - val_loss: 13.3766\n",
      "Epoch 36/1000\n",
      "\u001b[1m8036/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 21.9038\n",
      "Epoch 36: val_loss did not improve from 13.37655\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 679us/step - loss: 21.8396 - val_loss: 13.7665\n",
      "Epoch 37/1000\n",
      "\u001b[1m8020/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 21.9054\n",
      "Epoch 37: val_loss did not improve from 13.37655\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 651us/step - loss: 21.8252 - val_loss: 13.3771\n",
      "Epoch 38/1000\n",
      "\u001b[1m8053/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 641us/step - loss: 21.6073\n",
      "Epoch 38: val_loss did not improve from 13.37655\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 667us/step - loss: 21.5608 - val_loss: 13.5114\n",
      "Epoch 39/1000\n",
      "\u001b[1m8079/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 721us/step - loss: 21.4100\n",
      "Epoch 39: val_loss improved from 13.37655 to 13.01598, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 750us/step - loss: 21.3895 - val_loss: 13.0160\n",
      "Epoch 40/1000\n",
      "\u001b[1m8072/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 717us/step - loss: 21.4401\n",
      "Epoch 40: val_loss did not improve from 13.01598\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 747us/step - loss: 21.4127 - val_loss: 13.1480\n",
      "Epoch 41/1000\n",
      "\u001b[1m8063/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 21.3270\n",
      "Epoch 41: val_loss did not improve from 13.01598\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 697us/step - loss: 21.2911 - val_loss: 13.1751\n",
      "Epoch 42/1000\n",
      "\u001b[1m8035/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 21.1195\n",
      "Epoch 42: val_loss improved from 13.01598 to 12.99090, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 707us/step - loss: 21.0569 - val_loss: 12.9909\n",
      "Epoch 43/1000\n",
      "\u001b[1m8064/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 21.2683\n",
      "Epoch 43: val_loss did not improve from 12.99090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 700us/step - loss: 21.2331 - val_loss: 13.0773\n",
      "Epoch 44/1000\n",
      "\u001b[1m8012/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 21.4063\n",
      "Epoch 44: val_loss did not improve from 12.99090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 708us/step - loss: 21.3193 - val_loss: 13.4589\n",
      "Epoch 45/1000\n",
      "\u001b[1m8033/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 21.1086\n",
      "Epoch 45: val_loss improved from 12.99090 to 12.75175, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 670us/step - loss: 21.0431 - val_loss: 12.7518\n",
      "Epoch 46/1000\n",
      "\u001b[1m8046/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 21.1836\n",
      "Epoch 46: val_loss did not improve from 12.75175\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 647us/step - loss: 21.1307 - val_loss: 13.1245\n",
      "Epoch 47/1000\n",
      "\u001b[1m8040/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 21.1726\n",
      "Epoch 47: val_loss improved from 12.75175 to 12.59366, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 659us/step - loss: 21.1143 - val_loss: 12.5937\n",
      "Epoch 48/1000\n",
      "\u001b[1m8093/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 21.1439\n",
      "Epoch 48: val_loss improved from 12.59366 to 12.43314, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 659us/step - loss: 21.1371 - val_loss: 12.4331\n",
      "Epoch 49/1000\n",
      "\u001b[1m8027/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 20.9691\n",
      "Epoch 49: val_loss did not improve from 12.43314\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 646us/step - loss: 20.8989 - val_loss: 12.7341\n",
      "Epoch 50/1000\n",
      "\u001b[1m8057/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 21.0106\n",
      "Epoch 50: val_loss improved from 12.43314 to 12.36748, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 654us/step - loss: 20.9690 - val_loss: 12.3675\n",
      "Epoch 51/1000\n",
      "\u001b[1m8063/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 681us/step - loss: 21.0161\n",
      "Epoch 51: val_loss did not improve from 12.36748\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 705us/step - loss: 20.9805 - val_loss: 12.3951\n",
      "Epoch 52/1000\n",
      "\u001b[1m8063/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 20.9871\n",
      "Epoch 52: val_loss did not improve from 12.36748\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 684us/step - loss: 20.9512 - val_loss: 12.4041\n",
      "Epoch 53/1000\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 20.9017\n",
      "Epoch 53: val_loss improved from 12.36748 to 12.11119, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 665us/step - loss: 20.9008 - val_loss: 12.1112\n",
      "Epoch 54/1000\n",
      "\u001b[1m8027/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 20.8261\n",
      "Epoch 54: val_loss improved from 12.11119 to 11.99515, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 705us/step - loss: 20.7557 - val_loss: 11.9952\n",
      "Epoch 55/1000\n",
      "\u001b[1m8060/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 20.7081\n",
      "Epoch 55: val_loss did not improve from 11.99515\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 670us/step - loss: 20.6699 - val_loss: 12.5989\n",
      "Epoch 56/1000\n",
      "\u001b[1m8029/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 20.6309\n",
      "Epoch 56: val_loss did not improve from 11.99515\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 651us/step - loss: 20.5637 - val_loss: 12.3226\n",
      "Epoch 57/1000\n",
      "\u001b[1m8064/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 20.8672\n",
      "Epoch 57: val_loss did not improve from 11.99515\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 655us/step - loss: 20.8325 - val_loss: 12.0890\n",
      "Epoch 58/1000\n",
      "\u001b[1m8071/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 20.3586\n",
      "Epoch 58: val_loss did not improve from 11.99515\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 653us/step - loss: 20.3315 - val_loss: 12.2682\n",
      "Epoch 59/1000\n",
      "\u001b[1m8009/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 20.6726\n",
      "Epoch 59: val_loss did not improve from 11.99515\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 668us/step - loss: 20.5860 - val_loss: 12.2204\n",
      "Epoch 60/1000\n",
      "\u001b[1m8048/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 20.8121\n",
      "Epoch 60: val_loss improved from 11.99515 to 11.83951, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 705us/step - loss: 20.7620 - val_loss: 11.8395\n",
      "Epoch 61/1000\n",
      "\u001b[1m8089/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 20.3392\n",
      "Epoch 61: val_loss did not improve from 11.83951\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 665us/step - loss: 20.3289 - val_loss: 12.2019\n",
      "Epoch 62/1000\n",
      "\u001b[1m8046/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 20.1971\n",
      "Epoch 62: val_loss improved from 11.83951 to 11.70819, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 653us/step - loss: 20.1470 - val_loss: 11.7082\n",
      "Epoch 63/1000\n",
      "\u001b[1m8065/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 20.3381\n",
      "Epoch 63: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 658us/step - loss: 20.3052 - val_loss: 12.3204\n",
      "Epoch 64/1000\n",
      "\u001b[1m8036/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 20.2152\n",
      "Epoch 64: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 673us/step - loss: 20.1558 - val_loss: 12.1513\n",
      "Epoch 65/1000\n",
      "\u001b[1m8040/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 20.1768\n",
      "Epoch 65: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 753us/step - loss: 20.1212 - val_loss: 12.0165\n",
      "Epoch 66/1000\n",
      "\u001b[1m8044/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 623us/step - loss: 20.3873\n",
      "Epoch 66: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 651us/step - loss: 20.3344 - val_loss: 11.9557\n",
      "Epoch 67/1000\n",
      "\u001b[1m8042/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 638us/step - loss: 19.8017\n",
      "Epoch 67: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 667us/step - loss: 19.7488 - val_loss: 11.7131\n",
      "Epoch 68/1000\n",
      "\u001b[1m8024/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 19.9676\n",
      "Epoch 68: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 630us/step - loss: 19.8971 - val_loss: 11.8778\n",
      "Epoch 69/1000\n",
      "\u001b[1m8083/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 651us/step - loss: 19.8056\n",
      "Epoch 69: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 679us/step - loss: 19.7902 - val_loss: 11.9271\n",
      "Epoch 70/1000\n",
      "\u001b[1m8027/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 19.8899\n",
      "Epoch 70: val_loss did not improve from 11.70819\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 695us/step - loss: 19.8231 - val_loss: 11.9434\n",
      "Epoch 71/1000\n",
      "\u001b[1m8092/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 622us/step - loss: 19.8628\n",
      "Epoch 71: val_loss improved from 11.70819 to 11.29015, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 653us/step - loss: 19.8555 - val_loss: 11.2901\n",
      "Epoch 72/1000\n",
      "\u001b[1m8018/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 19.6925\n",
      "Epoch 72: val_loss did not improve from 11.29015\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 665us/step - loss: 19.6184 - val_loss: 11.4669\n",
      "Epoch 73/1000\n",
      "\u001b[1m8089/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 19.8012\n",
      "Epoch 73: val_loss did not improve from 11.29015\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 674us/step - loss: 19.7911 - val_loss: 11.3009\n",
      "Epoch 74/1000\n",
      "\u001b[1m8066/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 19.6595\n",
      "Epoch 74: val_loss did not improve from 11.29015\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 658us/step - loss: 19.6287 - val_loss: 11.4088\n",
      "Epoch 75/1000\n",
      "\u001b[1m8040/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 609us/step - loss: 19.7288\n",
      "Epoch 75: val_loss improved from 11.29015 to 11.12291, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 657us/step - loss: 19.6740 - val_loss: 11.1229\n",
      "Epoch 76/1000\n",
      "\u001b[1m8098/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 697us/step - loss: 19.6875\n",
      "Epoch 76: val_loss did not improve from 11.12291\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 723us/step - loss: 19.6857 - val_loss: 11.7830\n",
      "Epoch 77/1000\n",
      "\u001b[1m8074/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 19.0977\n",
      "Epoch 77: val_loss did not improve from 11.12291\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 724us/step - loss: 19.0749 - val_loss: 11.7841\n",
      "Epoch 78/1000\n",
      "\u001b[1m8056/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 19.4424\n",
      "Epoch 78: val_loss did not improve from 11.12291\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 712us/step - loss: 19.4034 - val_loss: 11.2330\n",
      "Epoch 79/1000\n",
      "\u001b[1m8047/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 19.4518\n",
      "Epoch 79: val_loss did not improve from 11.12291\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675us/step - loss: 19.4046 - val_loss: 11.5350\n",
      "Epoch 80/1000\n",
      "\u001b[1m8072/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 19.4437\n",
      "Epoch 80: val_loss did not improve from 11.12291\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 699us/step - loss: 19.4187 - val_loss: 11.8728\n",
      "Epoch 81/1000\n",
      "\u001b[1m8043/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 598us/step - loss: 19.1095\n",
      "Epoch 81: val_loss did not improve from 11.12291\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 627us/step - loss: 19.0605 - val_loss: 11.7707\n",
      "Epoch 82/1000\n",
      "\u001b[1m8082/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 699us/step - loss: 18.8061\n",
      "Epoch 82: val_loss improved from 11.12291 to 10.94767, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 726us/step - loss: 18.7907 - val_loss: 10.9477\n",
      "Epoch 83/1000\n",
      "\u001b[1m8076/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 645us/step - loss: 18.5649\n",
      "Epoch 83: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 675us/step - loss: 18.5447 - val_loss: 11.9322\n",
      "Epoch 84/1000\n",
      "\u001b[1m8087/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 693us/step - loss: 19.5435\n",
      "Epoch 84: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 726us/step - loss: 19.5317 - val_loss: 11.4700\n",
      "Epoch 85/1000\n",
      "\u001b[1m8056/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 19.0801\n",
      "Epoch 85: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 706us/step - loss: 19.0415 - val_loss: 12.1924\n",
      "Epoch 86/1000\n",
      "\u001b[1m8031/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 18.6859\n",
      "Epoch 86: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 702us/step - loss: 18.6269 - val_loss: 11.6421\n",
      "Epoch 87/1000\n",
      "\u001b[1m8098/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 620us/step - loss: 19.0149\n",
      "Epoch 87: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 646us/step - loss: 19.0131 - val_loss: 11.7006\n",
      "Epoch 88/1000\n",
      "\u001b[1m8044/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 18.2644\n",
      "Epoch 88: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 685us/step - loss: 18.2181 - val_loss: 11.5993\n",
      "Epoch 89/1000\n",
      "\u001b[1m8048/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 612us/step - loss: 18.9931\n",
      "Epoch 89: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 642us/step - loss: 18.9476 - val_loss: 11.8553\n",
      "Epoch 90/1000\n",
      "\u001b[1m8063/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 18.2613\n",
      "Epoch 90: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 657us/step - loss: 18.2305 - val_loss: 11.6149\n",
      "Epoch 91/1000\n",
      "\u001b[1m8027/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 19.0416\n",
      "Epoch 91: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 647us/step - loss: 18.9773 - val_loss: 11.5200\n",
      "Epoch 92/1000\n",
      "\u001b[1m8038/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 619us/step - loss: 18.5013\n",
      "Epoch 92: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 646us/step - loss: 18.4486 - val_loss: 11.4390\n",
      "Epoch 93/1000\n",
      "\u001b[1m8086/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 608us/step - loss: 18.2174\n",
      "Epoch 93: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 636us/step - loss: 18.2058 - val_loss: 11.8835\n",
      "Epoch 94/1000\n",
      "\u001b[1m8037/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 616us/step - loss: 18.5184\n",
      "Epoch 94: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 643us/step - loss: 18.4650 - val_loss: 11.5566\n",
      "Epoch 95/1000\n",
      "\u001b[1m8056/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 18.5528\n",
      "Epoch 95: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 641us/step - loss: 18.5149 - val_loss: 11.6526\n",
      "Epoch 96/1000\n",
      "\u001b[1m8097/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 615us/step - loss: 18.2390\n",
      "Epoch 96: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 641us/step - loss: 18.2365 - val_loss: 12.0105\n",
      "Epoch 97/1000\n",
      "\u001b[1m8031/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 624us/step - loss: 18.2609\n",
      "Epoch 97: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 668us/step - loss: 18.2035 - val_loss: 11.5605\n",
      "Epoch 98/1000\n",
      "\u001b[1m8070/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18.0823\n",
      "Epoch 98: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 18.0574 - val_loss: 11.4780\n",
      "Epoch 99/1000\n",
      "\u001b[1m8061/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 18.2835\n",
      "Epoch 99: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 18.2504 - val_loss: 11.8121\n",
      "Epoch 100/1000\n",
      "\u001b[1m8039/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 18.2317\n",
      "Epoch 100: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 669us/step - loss: 18.1803 - val_loss: 11.7980\n",
      "Epoch 101/1000\n",
      "\u001b[1m8058/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 18.3182\n",
      "Epoch 101: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 674us/step - loss: 18.2823 - val_loss: 11.6266\n",
      "Epoch 102/1000\n",
      "\u001b[1m8014/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 18.0629\n",
      "Epoch 102: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 667us/step - loss: 17.9918 - val_loss: 11.5490\n",
      "Epoch 103/1000\n",
      "\u001b[1m8056/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 18.1795\n",
      "Epoch 103: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 682us/step - loss: 18.1427 - val_loss: 11.4342\n",
      "Epoch 104/1000\n",
      "\u001b[1m8088/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 17.9642\n",
      "Epoch 104: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 683us/step - loss: 17.9542 - val_loss: 11.7860\n",
      "Epoch 105/1000\n",
      "\u001b[1m8050/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 17.9411\n",
      "Epoch 105: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 676us/step - loss: 17.8998 - val_loss: 11.2279\n",
      "Epoch 106/1000\n",
      "\u001b[1m8023/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 18.1393\n",
      "Epoch 106: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 674us/step - loss: 18.0738 - val_loss: 11.6553\n",
      "Epoch 107/1000\n",
      "\u001b[1m8079/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 18.1324\n",
      "Epoch 107: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 656us/step - loss: 18.1146 - val_loss: 12.0119\n",
      "Epoch 108/1000\n",
      "\u001b[1m8089/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 17.7550\n",
      "Epoch 108: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 672us/step - loss: 17.7460 - val_loss: 11.6488\n",
      "Epoch 109/1000\n",
      "\u001b[1m8067/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 686us/step - loss: 17.9755\n",
      "Epoch 109: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 715us/step - loss: 17.9482 - val_loss: 11.3128\n",
      "Epoch 110/1000\n",
      "\u001b[1m8076/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 17.7696\n",
      "Epoch 110: val_loss did not improve from 10.94767\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 678us/step - loss: 17.7501 - val_loss: 11.2619\n",
      "Epoch 111/1000\n",
      "\u001b[1m8095/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 17.8247\n",
      "Epoch 111: val_loss improved from 10.94767 to 10.86090, saving model to ./models/dense_64_32_11_mse_v1.2.keras\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 681us/step - loss: 17.8206 - val_loss: 10.8609\n",
      "Epoch 112/1000\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 18.4925\n",
      "Epoch 112: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 674us/step - loss: 18.4917 - val_loss: 11.4554\n",
      "Epoch 113/1000\n",
      "\u001b[1m8048/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 17.7897\n",
      "Epoch 113: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 794us/step - loss: 17.7471 - val_loss: 11.2574\n",
      "Epoch 114/1000\n",
      "\u001b[1m8088/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 698us/step - loss: 17.7343\n",
      "Epoch 114: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 727us/step - loss: 17.7244 - val_loss: 11.9514\n",
      "Epoch 115/1000\n",
      "\u001b[1m8072/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 17.2289\n",
      "Epoch 115: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 697us/step - loss: 17.2070 - val_loss: 11.5254\n",
      "Epoch 116/1000\n",
      "\u001b[1m8058/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 17.6936\n",
      "Epoch 116: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 658us/step - loss: 17.6592 - val_loss: 12.0558\n",
      "Epoch 117/1000\n",
      "\u001b[1m8070/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 630us/step - loss: 17.7598\n",
      "Epoch 117: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 658us/step - loss: 17.7353 - val_loss: 11.4683\n",
      "Epoch 118/1000\n",
      "\u001b[1m8057/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 17.5670\n",
      "Epoch 118: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 685us/step - loss: 17.5321 - val_loss: 11.4461\n",
      "Epoch 119/1000\n",
      "\u001b[1m8040/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 17.4386\n",
      "Epoch 119: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 677us/step - loss: 17.3905 - val_loss: 10.9322\n",
      "Epoch 120/1000\n",
      "\u001b[1m8057/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 17.2326\n",
      "Epoch 120: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 708us/step - loss: 17.1989 - val_loss: 12.0800\n",
      "Epoch 121/1000\n",
      "\u001b[1m8080/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 17.3462\n",
      "Epoch 121: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 660us/step - loss: 17.3304 - val_loss: 11.7884\n",
      "Epoch 122/1000\n",
      "\u001b[1m8076/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 17.3637\n",
      "Epoch 122: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 719us/step - loss: 17.3446 - val_loss: 11.3974\n",
      "Epoch 123/1000\n",
      "\u001b[1m8030/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 723us/step - loss: 17.0591\n",
      "Epoch 123: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 749us/step - loss: 17.0047 - val_loss: 11.0992\n",
      "Epoch 124/1000\n",
      "\u001b[1m8086/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 678us/step - loss: 17.1073\n",
      "Epoch 124: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 706us/step - loss: 17.0962 - val_loss: 12.3781\n",
      "Epoch 125/1000\n",
      "\u001b[1m8050/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 17.6256\n",
      "Epoch 125: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 718us/step - loss: 17.5845 - val_loss: 11.2436\n",
      "Epoch 126/1000\n",
      "\u001b[1m8022/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 17.3472\n",
      "Epoch 126: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 682us/step - loss: 17.2845 - val_loss: 11.7269\n",
      "Epoch 127/1000\n",
      "\u001b[1m8060/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 621us/step - loss: 16.7914\n",
      "Epoch 127: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 647us/step - loss: 16.7606 - val_loss: 10.9366\n",
      "Epoch 128/1000\n",
      "\u001b[1m8039/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 643us/step - loss: 17.0379\n",
      "Epoch 128: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 671us/step - loss: 16.9898 - val_loss: 12.1883\n",
      "Epoch 129/1000\n",
      "\u001b[1m8032/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 17.1018\n",
      "Epoch 129: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 662us/step - loss: 17.0480 - val_loss: 11.8922\n",
      "Epoch 130/1000\n",
      "\u001b[1m8058/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 631us/step - loss: 17.3297\n",
      "Epoch 130: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 661us/step - loss: 17.2960 - val_loss: 10.8646\n",
      "Epoch 131/1000\n",
      "\u001b[1m8096/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 626us/step - loss: 16.8243\n",
      "Epoch 131: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 652us/step - loss: 16.8212 - val_loss: 10.9821\n",
      "Epoch 132/1000\n",
      "\u001b[1m8062/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 747us/step - loss: 16.6487\n",
      "Epoch 132: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 774us/step - loss: 16.6198 - val_loss: 12.1080\n",
      "Epoch 133/1000\n",
      "\u001b[1m8032/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 17.1343\n",
      "Epoch 133: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 672us/step - loss: 17.0803 - val_loss: 11.7832\n",
      "Epoch 134/1000\n",
      "\u001b[1m8032/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 625us/step - loss: 16.9099\n",
      "Epoch 134: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 650us/step - loss: 16.8575 - val_loss: 13.3584\n",
      "Epoch 135/1000\n",
      "\u001b[1m8056/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 637us/step - loss: 16.8018\n",
      "Epoch 135: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 666us/step - loss: 16.7679 - val_loss: 13.1244\n",
      "Epoch 136/1000\n",
      "\u001b[1m8029/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 627us/step - loss: 17.3191\n",
      "Epoch 136: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 650us/step - loss: 17.2618 - val_loss: 12.7262\n",
      "Epoch 137/1000\n",
      "\u001b[1m8065/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 16.7797\n",
      "Epoch 137: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 697us/step - loss: 16.7531 - val_loss: 13.6260\n",
      "Epoch 138/1000\n",
      "\u001b[1m8067/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 16.5636\n",
      "Epoch 138: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 693us/step - loss: 16.5382 - val_loss: 13.5668\n",
      "Epoch 139/1000\n",
      "\u001b[1m8015/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 701us/step - loss: 17.0093\n",
      "Epoch 139: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 723us/step - loss: 16.9427 - val_loss: 13.3434\n",
      "Epoch 140/1000\n",
      "\u001b[1m8087/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 17.7292\n",
      "Epoch 140: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 696us/step - loss: 17.7183 - val_loss: 13.1805\n",
      "Epoch 141/1000\n",
      "\u001b[1m8078/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 659us/step - loss: 17.3017\n",
      "Epoch 141: val_loss did not improve from 10.86090\n",
      "\u001b[1m8099/8099\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 689us/step - loss: 17.2836 - val_loss: 14.1786\n"
     ]
    }
   ],
   "source": [
    "if new_train:\n",
    "    # Set model train seed\n",
    "    set_random_seed(42)\n",
    "\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=1000, validation_data=(X_val_scaled, y_val), callbacks=[early_stopping_cb, model_checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symetric_mape(A, F):\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error (SMAPE) metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np.array\n",
    "        Actual values.\n",
    "\n",
    "    F : np.array\n",
    "        Forecasted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        SMAPE metric value.\n",
    "    \n",
    "    \"\"\"\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference values for MAPE and SMAPE\n",
    "\n",
    "| MAPE Value | SMAPE Value | Predictive Performance Evaluation\n",
    "| :-: | :-: | :-: |\n",
    "| < 10% | <10% | Highly accurate forecasting |\n",
    "| 10-20% | 10-20% | Good forecasting |\n",
    "| 20-50% | 20-50% | Reasonable forecasting |\n",
    "| >50% | >50% | Inaccurate forecasting |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall metrics:\n",
      "R2: 0.9805410490090405\n",
      "MAPE: 369115749862681.44\n",
      "Shifted MAPE: 98.65581935093451\n",
      "Symetric MAPE: 91.15422872398636\n",
      "MSE: 3.7583571392337385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "print('Overall metrics:')\n",
    "print('R2:', r2_score(y_test, y_pred))\n",
    "print('MAPE:', mean_absolute_percentage_error(y_test, y_pred))\n",
    "print('Shifted MAPE:', np.mean(shifted_mape(y_test, y_pred)))\n",
    "print('Symetric MAPE:', np.mean(symetric_mape(y_test, y_pred)))\n",
    "print('MSE:', np.mean((y_test - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mape</th>\n",
       "      <th>smape</th>\n",
       "      <th>shifted mape</th>\n",
       "      <th>r2</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>consumoEspecificoTG1_2</th>\n",
       "      <td>1.486349e+14</td>\n",
       "      <td>100.244385</td>\n",
       "      <td>109.835402</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.020197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumoEspecificoTG2_2</th>\n",
       "      <td>1.904107e+14</td>\n",
       "      <td>102.186390</td>\n",
       "      <td>106.638037</td>\n",
       "      <td>0.999338</td>\n",
       "      <td>0.022514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potenciaGeradaTG2_2</th>\n",
       "      <td>2.479268e+14</td>\n",
       "      <td>104.263931</td>\n",
       "      <td>116.456995</td>\n",
       "      <td>0.996233</td>\n",
       "      <td>0.034226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumoEspecificoTG2_1</th>\n",
       "      <td>2.170767e+14</td>\n",
       "      <td>103.214952</td>\n",
       "      <td>116.922971</td>\n",
       "      <td>0.990021</td>\n",
       "      <td>0.078247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potenciaGeradaTG1_2</th>\n",
       "      <td>3.259803e+14</td>\n",
       "      <td>100.168433</td>\n",
       "      <td>109.688993</td>\n",
       "      <td>0.999210</td>\n",
       "      <td>0.080970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potenciaGeradaTG2_1</th>\n",
       "      <td>3.418321e+14</td>\n",
       "      <td>103.122653</td>\n",
       "      <td>114.356227</td>\n",
       "      <td>0.998415</td>\n",
       "      <td>0.131863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>potenciaGeradaTG1_1</th>\n",
       "      <td>5.135414e+14</td>\n",
       "      <td>101.295703</td>\n",
       "      <td>105.145200</td>\n",
       "      <td>0.998763</td>\n",
       "      <td>0.190267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vazaoVaporEscape</th>\n",
       "      <td>4.732285e-03</td>\n",
       "      <td>0.471885</td>\n",
       "      <td>1.035810</td>\n",
       "      <td>0.998949</td>\n",
       "      <td>5.067501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consumoEspecificoTG1_1</th>\n",
       "      <td>6.353914e+14</td>\n",
       "      <td>104.495129</td>\n",
       "      <td>110.060008</td>\n",
       "      <td>0.919640</td>\n",
       "      <td>15.574207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                mape       smape  shifted mape        r2  \\\n",
       "consumoEspecificoTG1_2  1.486349e+14  100.244385    109.835402  0.999116   \n",
       "consumoEspecificoTG2_2  1.904107e+14  102.186390    106.638037  0.999338   \n",
       "potenciaGeradaTG2_2     2.479268e+14  104.263931    116.456995  0.996233   \n",
       "consumoEspecificoTG2_1  2.170767e+14  103.214952    116.922971  0.990021   \n",
       "potenciaGeradaTG1_2     3.259803e+14  100.168433    109.688993  0.999210   \n",
       "potenciaGeradaTG2_1     3.418321e+14  103.122653    114.356227  0.998415   \n",
       "potenciaGeradaTG1_1     5.135414e+14  101.295703    105.145200  0.998763   \n",
       "vazaoVaporEscape        4.732285e-03    0.471885      1.035810  0.998949   \n",
       "consumoEspecificoTG1_1  6.353914e+14  104.495129    110.060008  0.919640   \n",
       "\n",
       "                              mse  \n",
       "consumoEspecificoTG1_2   0.020197  \n",
       "consumoEspecificoTG2_2   0.022514  \n",
       "potenciaGeradaTG2_2      0.034226  \n",
       "consumoEspecificoTG2_1   0.078247  \n",
       "potenciaGeradaTG1_2      0.080970  \n",
       "potenciaGeradaTG2_1      0.131863  \n",
       "potenciaGeradaTG1_1      0.190267  \n",
       "vazaoVaporEscape         5.067501  \n",
       "consumoEspecificoTG1_1  15.574207  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smape = np.array([symetric_mape(y_test[target], y_pred[:, i]) for i, target in enumerate(targets)])\n",
    "mape = np.array([mean_absolute_percentage_error(y_test[target], y_pred[:, i]) for i, target in enumerate(targets)])\n",
    "r_2 = np.array([r2_score(y_test[target], y_pred[:, i]) for i, target in enumerate(targets)])\n",
    "mse = np.array([np.mean((y_test[target] - y_pred[:, i])**2) for i, target in enumerate(targets)])\n",
    "\n",
    "# Create a DataFrame with the MAPE for each target\n",
    "mape_df = pd.DataFrame({'mape': mape, 'smape': smape, 'shifted mape': shif_mape, 'r2': r_2, 'mse': mse}, index=targets).sort_values('mse')\n",
    "mape_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
